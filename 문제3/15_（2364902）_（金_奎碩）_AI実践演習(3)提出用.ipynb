{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0PUOIFctkg-"
      },
      "source": [
        "# AI実践演習(3)提出用\n",
        "* 学籍番号：2364902\n",
        "* 氏名：金 奎碩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOlSSpYaHWW1"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 演習(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFlzZTMFJCWA"
      },
      "source": [
        "### **取り組む問題の説明**\n",
        "昔話題になったAlphaGoの深層強化学習（MCTS + Policy Network + Value Network + Self-Play）を利用して五目並べのAIを学習させてユーザと対戦を行う。\n",
        "#### **概要・目的**\n",
        "深層強化学習とモンテカルロ木探索（MCTS）を活用して、五目並べ（Gomoku）AI を構築する。このAIは、ニューラルネットワークを用いたポリシー・バリューネットワークを利用し、自己対局を通じて強化学習を行い、最適な手を探索できるようにする。そして、学習の量によって違うモデルで対戦を行うことでより強いAIを確認する。\n",
        "\n",
        "##### 具体的な目的\n",
        "1. ゲーム環境の構築：7×7の盤面で、5個の石が並んだら勝利となる五目並べ環境を構築。\n",
        "2. ニューラルネットワークの設計：畳み込みニューラルネットワーク（CNN）を利用して、ポリシーと価値の予測を行うモデルを構築。\n",
        "3. モンテカルロ木探索（MCTS）の実装：ニューラルネットワークの出力を基に、シミュレーションを行い、最適な手を探索\n",
        "4. 自己対局によるデータ収集と学習：自己対局を通じてデータを収集し、ニューラルネットワークを学習。\n",
        "5.  AIと対戦できるインターフェースの提供：学習したAIと対戦できる機能を実装。\n",
        "#### **使用するデータセット**\n",
        "自己対局によるデータ を生成し、それを学習する。外部のデータセットは使用せず、以下のようなデータを収集する予定。\n",
        "* 各ゲームの 状態（盤面の配置）\n",
        "* 各状態での モンテカルロ木探索（MCTS）による行動確率\n",
        "* ゲームの結果（勝ち・負け・引き分け）\n",
        "\n",
        "#### **学習するモデル**\n",
        "* ニューラルネットワーク構造\n",
        "畳み込みニューラルネットワーク（CNN）を利用したポリシー・バリューネットワーク を構築する。\n",
        "\n",
        "  このネットワークは、以下の2つの役割を持っている。\n",
        "  1. ポリシーネットワーク（Policy Network）：各状態における次の手の確率分布を予測。\n",
        "  2. バリューネットワーク（Value Network）：その状態の勝率（評価値）を予測。\n",
        "* モンテカルロ木探索（MCTS）\n",
        "  * ノードの選択（Selection）：UCB1アルゴリズムを利用し、探索と活用のバランスを取る。\n",
        "  * シミュレーション（Simulation）：ランダムプレイアウトの代わりに、ニューラルネットワークの出力を利用する。\n",
        "  * バックプロパゲーション（Backpropagation）：各ノードのQ値と訪問回数を更新する。\n",
        "\n",
        "\n",
        "#### **パラメータの決め方**\n",
        "| **ハイパーパラメータ** | **値** | **説明** |\n",
        "|-----------------|------|--------------|\n",
        "| `BOARD_SIZE` | `7` | 盤面のサイズ（7×7） |\n",
        "| `WIN_LENGTH` | `5` | 5個連続で並べたら勝利 |\n",
        "| `c_puct` | `1.0` | MCTSの探索パラメータ |\n",
        "| `n_simulations` | `1000` | 1手ごとのMCTSシミュレーション回数 |\n",
        "| `learning_rate` | `0.001` | Adamオプティマイザの学習率 |\n",
        "| `epochs` | `100` | 学習時のエポック数 |\n",
        "| `batch_size` | `32` | ミニバッチサイズ |\n",
        "| `num_self_play_games` | `20` | 自己対局の回数 |\n",
        "\n",
        "#### **参考サイトのURL（あれば）**\n",
        "1. https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf\n",
        "2. https://qiita.com/toyohisa/items/e9f218909214c3a98ce2?utm_source=chatgpt.com\n",
        "3. https://qiita.com/pocokhc/items/392a7f89c79f5e1e6bca?utm_source=chatgpt.com\n",
        "4. https://www.brainpad.co.jp/doors/contents/01_tech_2018-04-05-163000/?utm_source=chatgpt.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ゲーム環境とヘルパー関数"
      ],
      "metadata": {
        "id": "5BjG25JVbt0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "73UlT1PGZwN6"
      },
      "outputs": [],
      "source": [
        "BOARD_SIZE = 7\n",
        "WIN_LENGTH = 5\n",
        "\n",
        "def check_win(board, player):\n",
        "    # 指定されたプレイヤーの石が横、縦、または斜めに WIN_LENGTH 以上連続しているか確認\n",
        "    for i in range(BOARD_SIZE):\n",
        "        for j in range(BOARD_SIZE):\n",
        "            if board[i, j] == player:\n",
        "                # 横のチェック\n",
        "                if j <= BOARD_SIZE - WIN_LENGTH and all(board[i, j + k] == player for k in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "                # 縦のチェック\n",
        "                if i <= BOARD_SIZE - WIN_LENGTH and all(board[i + k, j] == player for k in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "                # 斜め（右下）\n",
        "                if i <= BOARD_SIZE - WIN_LENGTH and j <= BOARD_SIZE - WIN_LENGTH and all(board[i + k, j + k] == player for k in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "                # 斜め（左下）\n",
        "                if i >= WIN_LENGTH - 1 and j <= BOARD_SIZE - WIN_LENGTH and all(board[i - k, j + k] == player for k in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "    return False\n",
        "\n",
        "def get_valid_moves(state):\n",
        "    board, _ = state\n",
        "    valid = []\n",
        "    for i in range(BOARD_SIZE):\n",
        "        for j in range(BOARD_SIZE):\n",
        "            if board[i, j] == 0:\n",
        "                valid.append(i * BOARD_SIZE + j)\n",
        "    return valid\n",
        "\n",
        "def get_next_state(state, action):\n",
        "    board, player = state\n",
        "    new_board = board.copy()\n",
        "    row = action // BOARD_SIZE\n",
        "    col = action % BOARD_SIZE\n",
        "    new_board[row, col] = player\n",
        "    return (new_board, -player)\n",
        "\n",
        "def serialize_state(state):\n",
        "    board, player = state\n",
        "    # 現在のプレイヤー情報を1バイトに保存\n",
        "    return board.tobytes() + (b'\\x00' if player == 1 else b'\\x01')\n",
        "\n",
        "def get_game_result(state):\n",
        "    board, player = state\n",
        "    # 最後に置いた石が勝利条件を満たすか確認\n",
        "    if check_win(board, player):  # <- 元の -player を player に変更\n",
        "        return 1  # 勝利\n",
        "\n",
        "    if len(get_valid_moves(state)) == 0:\n",
        "        return 0  # 引き分け\n",
        "\n",
        "    return None  # ゲーム進行中\n",
        "\n",
        "\n",
        "def state_to_tensor(state):\n",
        "    board, player = state\n",
        "    tensor = np.zeros((2, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n",
        "    tensor[0] = (board == player).astype(np.float32)\n",
        "    tensor[1] = (board == -player).astype(np.float32)\n",
        "    return torch.tensor(tensor)\n",
        "\n",
        "def print_board(board):\n",
        "    for i in range(BOARD_SIZE):\n",
        "        line = \"\"\n",
        "        for j in range(BOARD_SIZE):\n",
        "            if board[i, j] == 1:\n",
        "                line += \"X \"\n",
        "            elif board[i, j] == -1:\n",
        "                line += \"O \"\n",
        "            else:\n",
        "                line += \". \"\n",
        "        print(line)\n",
        "    print()  # 空行を追加\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. ニューラルネットワークモデル（Policy & Value）"
      ],
      "metadata": {
        "id": "27iIk6uIch-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GomokuNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GomokuNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * BOARD_SIZE * BOARD_SIZE, 128)\n",
        "        self.fc_policy = nn.Linear(128, BOARD_SIZE * BOARD_SIZE)\n",
        "        self.fc_value = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, 64 * BOARD_SIZE * BOARD_SIZE)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        policy = self.fc_policy(x)\n",
        "        value = torch.tanh(self.fc_value(x))\n",
        "        return policy, value"
      ],
      "metadata": {
        "id": "u7HUyL5gbEQe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. MCTSの実装"
      ],
      "metadata": {
        "id": "r7HMSC1ZcUnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTS:\n",
        "    def __init__(self, net, c_puct=1.0, n_simulations=1000):\n",
        "        self.net = net\n",
        "        self.c_puct = c_puct\n",
        "        self.n_simulations = n_simulations\n",
        "        self.Qsa = {}  # (state, action)ごとの Q値\n",
        "        self.Nsa = {}  # (state, action)ごとの訪問回数\n",
        "        self.Ns = {}   # stateごとの訪問回数\n",
        "        self.Ps = {}   # stateごとの事前確率（ニューラルネットワークの予測）\n",
        "        self.Es = {}   # stateの終了状態と結果\n",
        "        self.Vs = {}   # stateで可能な行動のリスト\n",
        "\n",
        "    def get_action_probs(self, state, temp=1):\n",
        "        for _ in range(self.n_simulations):\n",
        "            self.search(state)\n",
        "        s = serialize_state(state)\n",
        "        counts = [self.Nsa.get((s, a), 0) for a in range(BOARD_SIZE * BOARD_SIZE)]\n",
        "        if temp == 0:\n",
        "            bestA = np.argmax(counts)\n",
        "            probs = [0] * (BOARD_SIZE * BOARD_SIZE)\n",
        "            probs[bestA] = 1\n",
        "            return probs\n",
        "        counts = [x ** (1. / temp) for x in counts]\n",
        "        total = float(sum(counts))\n",
        "        probs = [x / total for x in counts]\n",
        "        return probs\n",
        "\n",
        "    def search(self, state):\n",
        "        s = serialize_state(state)\n",
        "        result = get_game_result(state)\n",
        "        if result is not None:\n",
        "            # 終端状態: 現在のプレイヤーの視点で結果を返す\n",
        "            return -result\n",
        "\n",
        "        if s not in self.Ps:\n",
        "            # リーフノードに到達: ニューラルネットワークで評価およびポリシー予測\n",
        "            valid_moves = get_valid_moves(state)\n",
        "            board_tensor = state_to_tensor(state).unsqueeze(0)  # バッチ次元を追加\n",
        "            self.net.eval()\n",
        "            with torch.no_grad():\n",
        "                logits, value = self.net(board_tensor)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy().flatten().tolist()\n",
        "            # 無効な手の確率を0に設定\n",
        "            for a in range(BOARD_SIZE * BOARD_SIZE):\n",
        "                if a not in valid_moves:\n",
        "                    probs[a] = 0\n",
        "            sum_probs = sum(probs)\n",
        "            if sum_probs > 0:\n",
        "                probs = [p / sum_probs for p in probs]\n",
        "            else:\n",
        "                # すべての確率が0の場合、有効な手に対して均等分布を適用\n",
        "                probs = [1 / len(valid_moves) if a in valid_moves else 0 for a in range(BOARD_SIZE * BOARD_SIZE)]\n",
        "            self.Ps[s] = probs\n",
        "            self.Vs[s] = valid_moves\n",
        "            self.Ns[s] = 0\n",
        "            return -value.item()\n",
        "\n",
        "        # 選択: UCBを使用して最適な行動を選択\n",
        "        best_ucb = -float('inf')\n",
        "        best_action = -1\n",
        "        for a in self.Vs[s]:\n",
        "            if (s, a) in self.Qsa:\n",
        "                ucb = self.Qsa[(s, a)] + self.c_puct * self.Ps[s][a] * np.sqrt(self.Ns[s]) / (1 + self.Nsa[(s, a)])\n",
        "            else:\n",
        "                ucb = self.c_puct * self.Ps[s][a] * np.sqrt(self.Ns[s] + 1e-8)\n",
        "            if ucb > best_ucb:\n",
        "                best_ucb = ucb\n",
        "                best_action = a\n",
        "        a = best_action\n",
        "        next_state = get_next_state(state, a)\n",
        "        v = self.search(next_state)\n",
        "        # バックアップ: Q, Nの更新\n",
        "        if (s, a) in self.Qsa:\n",
        "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
        "            self.Nsa[(s, a)] += 1\n",
        "        else:\n",
        "            self.Qsa[(s, a)] = v\n",
        "            self.Nsa[(s, a)] = 1\n",
        "        self.Ns[s] += 1\n",
        "        return -v\n"
      ],
      "metadata": {
        "id": "UkKyMoIYbJGa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 自己対局とネットワーク学習"
      ],
      "metadata": {
        "id": "gJZvDE3xdNpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_play_game(net, n_simulations=1000):\n",
        "    mcts = MCTS(net, n_simulations=n_simulations)\n",
        "    train_examples = []\n",
        "    state = (np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8), 1)\n",
        "    while True:\n",
        "        # 序盤は探索の温度を高く設定 (temp=1)、終盤は決定論的に (temp=0)\n",
        "        temp = 1 if len(train_examples) < 10 else 0\n",
        "        action_probs = mcts.get_action_probs(state, temp=temp)\n",
        "        train_examples.append((state, action_probs, None))\n",
        "        # 行動選択: 確率に基づいてサンプリング\n",
        "        action = np.random.choice(range(BOARD_SIZE * BOARD_SIZE), p=action_probs)\n",
        "        state = get_next_state(state, action)\n",
        "        result = get_game_result(state)\n",
        "        if result is not None:\n",
        "            # ゲーム終了: 各手に結果を付与 (プレイヤー交代に伴う符号の変更)\n",
        "            return [(s, pi, result * ((-1) ** i)) for i, (s, pi, _) in enumerate(train_examples)]\n",
        "\n",
        "def train_network(net, examples, epochs=10, batch_size=64, lr=0.001):\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        random.shuffle(examples)\n",
        "        for i in range(0, len(examples), batch_size):\n",
        "            batch = examples[i:i + batch_size]\n",
        "            states, target_pis, target_vs = zip(*batch)\n",
        "            state_tensors = torch.stack([state_to_tensor(s) for s in states])\n",
        "            target_pis = torch.tensor(target_pis, dtype=torch.float32)\n",
        "            target_vs = torch.tensor(target_vs, dtype=torch.float32).view(-1, 1)\n",
        "            optimizer.zero_grad()\n",
        "            pred_logits, pred_vs = net(state_tensors)\n",
        "            loss_v = F.mse_loss(pred_vs, target_vs)\n",
        "            loss_pi = -torch.mean(torch.sum(target_pis * F.log_softmax(pred_logits, dim=1), dim=1))\n",
        "            loss = loss_v + loss_pi\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1} completed.\")"
      ],
      "metadata": {
        "id": "XkGFtGO5bL_G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. モデルの保存/読み込みとユーザー対戦"
      ],
      "metadata": {
        "id": "oqJdYbFwdda8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_against_ai(net, n_simulations=1000):\n",
        "    state = (np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8), 1)\n",
        "    while True:\n",
        "        board, player = state\n",
        "        print_board(board)\n",
        "        if player == 1:\n",
        "            # ユーザーのターン\n",
        "            move = input(\"行と列をスペースで区切って入力してください (例: 2 3): \")\n",
        "            try:\n",
        "                row, col = map(int, move.split())\n",
        "            except:\n",
        "                print(\"無効な入力です。もう一度入力してください。\")\n",
        "                continue\n",
        "            action = row * BOARD_SIZE + col\n",
        "            if board[row, col] != 0:\n",
        "                print(\"その位置はすでに使用されています。他の場所を選択してください。\")\n",
        "                continue\n",
        "        else:\n",
        "            # AIのターン: MCTSを使用して決定\n",
        "            mcts = MCTS(net, n_simulations=n_simulations)\n",
        "            action_probs = mcts.get_action_probs(state, temp=0)\n",
        "            action = np.argmax(action_probs)\n",
        "            ai_row, ai_col = action // BOARD_SIZE, action % BOARD_SIZE\n",
        "            print(f\"AIが {ai_row}, {ai_col} の位置に置きました。\")\n",
        "        state = get_next_state(state, action)\n",
        "        result = get_game_result(state)\n",
        "        if result is not None:\n",
        "            print_board(state[0])\n",
        "            if result == 0:\n",
        "                print(\"引き分けです。\")\n",
        "            else:\n",
        "                # 最後に石を置いたプレイヤーが勝利した状態なので、\n",
        "                # 現在のターン (player) が 1 の場合、相手が勝利したことになる\n",
        "                if player == 1:\n",
        "                    print(\"敗北しました。\")\n",
        "                else:\n",
        "                    print(\"勝利しました。\")\n",
        "            break"
      ],
      "metadata": {
        "id": "_vkFoNP9bP-j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 全体の実行"
      ],
      "metadata": {
        "id": "2bR8jiPSdofX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    net = GomokuNet()\n",
        "\n",
        "    # 自己対局による学習データの収集 (例: 50ゲーム)\n",
        "    examples = []\n",
        "    num_self_play_games = 50\n",
        "    print(\"自己対局を開始します...\")\n",
        "    for i in range(num_self_play_games):\n",
        "        game_examples = self_play_game(net, n_simulations=1000)\n",
        "        examples.extend(game_examples)\n",
        "        print(f\"ゲーム {i+1} 完了。\")\n",
        "\n",
        "    # 収集したデータを用いてネットワークを学習\n",
        "    print(\"ネットワークの学習を開始します...\")\n",
        "    train_network(net, examples, epochs=100, batch_size=32, lr=0.001)\n",
        "\n",
        "    # 学習済みモデルを保存\n",
        "    torch.save(net.state_dict(), \"omok_model1.pth\")\n",
        "    print(\"モデルが保存されました: omok_model1.pth\")\n",
        "\n",
        "    # 保存したモデルの読み込み (必要に応じて)\n",
        "    # net.load_state_dict(torch.load(\"omok_model.pth\"))\n",
        "\n",
        "    # ユーザーとAIの対戦\n",
        "    print(\"ユーザーとAIの対戦を開始します。\")\n",
        "    play_against_ai(net, n_simulations=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdmkDufXbTke",
        "outputId": "4d22614a-b5e3-44a7-c7dd-cdaf68b1edca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "自己対局を開始します...\n",
            "ゲーム 1 完了。\n",
            "ゲーム 2 完了。\n",
            "ゲーム 3 完了。\n",
            "ゲーム 4 完了。\n",
            "ゲーム 5 完了。\n",
            "ゲーム 6 完了。\n",
            "ゲーム 7 完了。\n",
            "ゲーム 8 完了。\n",
            "ゲーム 9 完了。\n",
            "ゲーム 10 完了。\n",
            "ゲーム 11 完了。\n",
            "ゲーム 12 完了。\n",
            "ゲーム 13 完了。\n",
            "ゲーム 14 完了。\n",
            "ゲーム 15 完了。\n",
            "ゲーム 16 完了。\n",
            "ゲーム 17 完了。\n",
            "ゲーム 18 完了。\n",
            "ゲーム 19 完了。\n",
            "ゲーム 20 完了。\n",
            "ゲーム 21 完了。\n",
            "ゲーム 22 完了。\n",
            "ゲーム 23 完了。\n",
            "ゲーム 24 完了。\n",
            "ゲーム 25 完了。\n",
            "ゲーム 26 完了。\n",
            "ゲーム 27 完了。\n",
            "ゲーム 28 完了。\n",
            "ゲーム 29 完了。\n",
            "ゲーム 30 完了。\n",
            "ゲーム 31 完了。\n",
            "ゲーム 32 完了。\n",
            "ゲーム 33 完了。\n",
            "ゲーム 34 完了。\n",
            "ゲーム 35 完了。\n",
            "ゲーム 36 完了。\n",
            "ゲーム 37 完了。\n",
            "ゲーム 38 完了。\n",
            "ゲーム 39 完了。\n",
            "ゲーム 40 完了。\n",
            "ゲーム 41 完了。\n",
            "ゲーム 42 完了。\n",
            "ゲーム 43 完了。\n",
            "ゲーム 44 完了。\n",
            "ゲーム 45 完了。\n",
            "ゲーム 46 完了。\n",
            "ゲーム 47 完了。\n",
            "ゲーム 48 完了。\n",
            "ゲーム 49 完了。\n",
            "ゲーム 50 完了。\n",
            "ネットワークの学習を開始します...\n",
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 completed.\n",
            "Epoch 11 completed.\n",
            "Epoch 12 completed.\n",
            "Epoch 13 completed.\n",
            "Epoch 14 completed.\n",
            "Epoch 15 completed.\n",
            "Epoch 16 completed.\n",
            "Epoch 17 completed.\n",
            "Epoch 18 completed.\n",
            "Epoch 19 completed.\n",
            "Epoch 20 completed.\n",
            "Epoch 21 completed.\n",
            "Epoch 22 completed.\n",
            "Epoch 23 completed.\n",
            "Epoch 24 completed.\n",
            "Epoch 25 completed.\n",
            "Epoch 26 completed.\n",
            "Epoch 27 completed.\n",
            "Epoch 28 completed.\n",
            "Epoch 29 completed.\n",
            "Epoch 30 completed.\n",
            "Epoch 31 completed.\n",
            "Epoch 32 completed.\n",
            "Epoch 33 completed.\n",
            "Epoch 34 completed.\n",
            "Epoch 35 completed.\n",
            "Epoch 36 completed.\n",
            "Epoch 37 completed.\n",
            "Epoch 38 completed.\n",
            "Epoch 39 completed.\n",
            "Epoch 40 completed.\n",
            "Epoch 41 completed.\n",
            "Epoch 42 completed.\n",
            "Epoch 43 completed.\n",
            "Epoch 44 completed.\n",
            "Epoch 45 completed.\n",
            "Epoch 46 completed.\n",
            "Epoch 47 completed.\n",
            "Epoch 48 completed.\n",
            "Epoch 49 completed.\n",
            "Epoch 50 completed.\n",
            "Epoch 51 completed.\n",
            "Epoch 52 completed.\n",
            "Epoch 53 completed.\n",
            "Epoch 54 completed.\n",
            "Epoch 55 completed.\n",
            "Epoch 56 completed.\n",
            "Epoch 57 completed.\n",
            "Epoch 58 completed.\n",
            "Epoch 59 completed.\n",
            "Epoch 60 completed.\n",
            "Epoch 61 completed.\n",
            "Epoch 62 completed.\n",
            "Epoch 63 completed.\n",
            "Epoch 64 completed.\n",
            "Epoch 65 completed.\n",
            "Epoch 66 completed.\n",
            "Epoch 67 completed.\n",
            "Epoch 68 completed.\n",
            "Epoch 69 completed.\n",
            "Epoch 70 completed.\n",
            "Epoch 71 completed.\n",
            "Epoch 72 completed.\n",
            "Epoch 73 completed.\n",
            "Epoch 74 completed.\n",
            "Epoch 75 completed.\n",
            "Epoch 76 completed.\n",
            "Epoch 77 completed.\n",
            "Epoch 78 completed.\n",
            "Epoch 79 completed.\n",
            "Epoch 80 completed.\n",
            "Epoch 81 completed.\n",
            "Epoch 82 completed.\n",
            "Epoch 83 completed.\n",
            "Epoch 84 completed.\n",
            "Epoch 85 completed.\n",
            "Epoch 86 completed.\n",
            "Epoch 87 completed.\n",
            "Epoch 88 completed.\n",
            "Epoch 89 completed.\n",
            "Epoch 90 completed.\n",
            "Epoch 91 completed.\n",
            "Epoch 92 completed.\n",
            "Epoch 93 completed.\n",
            "Epoch 94 completed.\n",
            "Epoch 95 completed.\n",
            "Epoch 96 completed.\n",
            "Epoch 97 completed.\n",
            "Epoch 98 completed.\n",
            "Epoch 99 completed.\n",
            "Epoch 100 completed.\n",
            "モデルが保存されました: omok_model1.pth\n",
            "ユーザーとAIの対戦を開始します。\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 0\n",
            "X . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 3, 4 の位置に置きました。\n",
            "X . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 1\n",
            "X X . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 1, 6 の位置に置きました。\n",
            "X X . . . . . \n",
            ". . . . . . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 2\n",
            "X X X . . . . \n",
            ". . . . . . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 6, 5 の位置に置きました。\n",
            "X X X . . . . \n",
            ". . . . . . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 3\n",
            "X X X X . . . \n",
            ". . . . . . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            "\n",
            "AIが 5, 1 の位置に置きました。\n",
            "X X X X . . . \n",
            ". . . . . . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". O . . . . . \n",
            ". . . . . O . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 4\n",
            "X X X X X . . \n",
            ". . . . . . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". O . . . . . \n",
            ". . . . . O . \n",
            "\n",
            "AIが 1, 4 の位置に置きました。\n",
            "X X X X X . . \n",
            ". . . . O . O \n",
            ". . . . . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". O . . . . . \n",
            ". . . . . O . \n",
            "\n",
            "勝利しました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AIと対戦(num_self_play_games = 50, n_simulations = 500）"
      ],
      "metadata": {
        "id": "5zEphfx8dxeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5geHss2ekaC",
        "outputId": "fd1b2a76-d60c-491f-f00a-bf02e18c0b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-c35433139b6b>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(\"omok_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ユーザーとAIの対戦を開始します。\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 1\n",
            ". X . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 4, 5 の位置に置きました。\n",
            ". X . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 2\n",
            ". X X . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 5, 0 の位置に置きました。\n",
            ". X X . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 3\n",
            ". X X X . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 2, 3 の位置に置きました。\n",
            ". X X X . . . \n",
            ". . . . . . . \n",
            ". . . O . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 4\n",
            ". X X X X . . \n",
            ". . . . . . . \n",
            ". . . O . . . \n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 3, 5 の位置に置きました。\n",
            ". X X X X . . \n",
            ". . . . . . . \n",
            ". . . O . . . \n",
            ". . . . . O . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 5\n",
            ". X X X X X . \n",
            ". . . . . . . \n",
            ". . . O . . . \n",
            ". . . . . O . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 2, 4 の位置に置きました。\n",
            ". X X X X X . \n",
            ". . . . . . . \n",
            ". . . O O . . \n",
            ". . . . . O . \n",
            ". . . . . O . \n",
            "O . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "勝利しました。\n"
          ]
        }
      ],
      "source": [
        "    # 保存したモデルの読み込み (必要に応じて)\n",
        "    net.load_state_dict(torch.load(\"omok_model.pth\"))\n",
        "\n",
        "    # ユーザーとAIの対戦\n",
        "    print(\"ユーザーとAIの対戦を開始します。\")\n",
        "    play_against_ai(net, n_simulations=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AIと対戦(num_self_play_games = 50, n_simulations = 1000）"
      ],
      "metadata": {
        "id": "JfK_Rzj6itod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # 保存したモデルの読み込み (必要に応じて)\n",
        "    net.load_state_dict(torch.load(\"omok_model1.pth\"))\n",
        "\n",
        "    # ユーザーとAIの対戦\n",
        "    print(\"ユーザーとAIの対戦を開始します。\")\n",
        "    play_against_ai(net, n_simulations=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP3dcq_birIN",
        "outputId": "c9a39068-7079-461b-abd7-88278e6cef46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-9f4ad595b7ff>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(\"omok_model1.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ユーザーとAIの対戦を開始します。\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 3 3\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . X . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 3, 1 の位置に置きました。\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". O . X . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 3 2\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". O X X . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 4, 4 の位置に置きました。\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". O X X . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 2 3\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . X . . . \n",
            ". O X X . . . \n",
            ". . . . O . . \n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 5, 1 の位置に置きました。\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . X . . . \n",
            ". O X X . . . \n",
            ". . . . O . . \n",
            ". O . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 3 4\n",
            ". . . . . . . \n",
            ". . . . . . . \n",
            ". . . X . . . \n",
            ". O X X X . . \n",
            ". . . . O . . \n",
            ". O . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 1, 5 の位置に置きました。\n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            ". . . X . . . \n",
            ". O X X X . . \n",
            ". . . . O . . \n",
            ". O . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 4 3\n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            ". . . X . . . \n",
            ". O X X X . . \n",
            ". . . X O . . \n",
            ". O . . . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 5, 3 の位置に置きました。\n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            ". . . X . . . \n",
            ". O X X X . . \n",
            ". . . X O . . \n",
            ". O . O . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 2 3\n",
            "その位置はすでに使用されています。他の場所を選択してください。\n",
            ". . . . . . . \n",
            ". . . . . O . \n",
            ". . . X . . . \n",
            ". O X X X . . \n",
            ". . . X O . . \n",
            ". O . O . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 1 3\n",
            ". . . . . . . \n",
            ". . . X . O . \n",
            ". . . X . . . \n",
            ". O X X X . . \n",
            ". . . X O . . \n",
            ". O . O . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 2, 4 の位置に置きました。\n",
            ". . . . . . . \n",
            ". . . X . O . \n",
            ". . . X O . . \n",
            ". O X X X . . \n",
            ". . . X O . . \n",
            ". O . O . . . \n",
            ". . . . . . . \n",
            "\n",
            "行と列をスペースで区切って入力してください (例: 2 3): 0 3\n",
            ". . . X . . . \n",
            ". . . X . O . \n",
            ". . . X O . . \n",
            ". O X X X . . \n",
            ". . . X O . . \n",
            ". O . O . . . \n",
            ". . . . . . . \n",
            "\n",
            "AIが 3, 0 の位置に置きました。\n",
            ". . . X . . . \n",
            ". . . X . O . \n",
            ". . . X O . . \n",
            "O O X X X . . \n",
            ". . . X O . . \n",
            ". O . O . . . \n",
            ". . . . . . . \n",
            "\n",
            "勝利しました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 考察\n",
        "実際に対戦を行ってみたが思ったより正確に作ることはできなかった。\n",
        "\n",
        "その理由としてはnum_self_play_games = 50, n_simulations = 1000の場合学習させるのに1時間がかかってる。しかし、五目並べの場合場合の数がたくさん存在するので多くの場合を学習させるためにはたくさんの学習が必要である。\n",
        "\n",
        "それに加えて、AlphaGoZeroの場合はスーパーコンピューターを利用して学習させるのでスーパーコンピューターを利用することはできないので一番五目並べが生じない位置に置くだけのAIになったと思う。\n",
        "\n"
      ],
      "metadata": {
        "id": "s45MehUyEGf-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sndRzPLjjpJ-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 参考文献\n",
        "* DeepMind、「Mastering the game of Go without human knowledge」、UCL Discovery、2017年、https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf、 （参照：2025-02-10）\n",
        "\n",
        "* Toyohisa、「モンテカルロ木探索による五目並べAI #Python」、Qiita、更新日不明、https://qiita.com/toyohisa/items/e9f218909214c3a98ce2?utm_source=chatgpt.com、 （参照：2025-02-10）\n",
        "\n",
        "* Pocokhc、「【強化学習】モンテカルロ木探索を解説・実装」、Qiita、更新日不明、https://qiita.com/pocokhc/items/392a7f89c79f5e1e6bca?utm_source=chatgpt.com、 （参照：2025-02-10）\n",
        "\n",
        "* BrainPad、「AlphaGoZeroでも重要な技術要素！ モンテカルロ木探索の入門」、BrainPad、2018年4月5日、https://www.brainpad.co.jp/doors/contents/01_tech_2018-04-05-163000/?utm_source=chatgpt.com、 （参照：2025-02-10）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHUWSfqcMYjm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 講義の感想"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yxKufGjNKff"
      },
      "source": [
        "### 良かった点、興味を持った内容など\n",
        "ニュースやネットでしか見てないAIを実際に実装することでAIに関する理解が深まった。\n",
        "\n",
        "そして、一番興味深く思った学習方法は強化学習である。\n",
        "\n",
        "AIが人間のように試行錯誤を繰り返しながら学習することと実際に目に見える所の学習ができる点である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7yQRHO2NKoZ"
      },
      "source": [
        "### 難しかった点、講義の運営方法に対する要望など\n",
        "情報工学は1年生からプログラミング演習でC言語と今回はJAVAのプログラミングの演習を行ったがPythonはしっかり学んでないため予め知識がない人はコードの理解が難しいと思う。\n",
        "\n",
        "現在はパラメータの理解と学習方法を理解することはできるが、細かいところにわからないことが多い。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}